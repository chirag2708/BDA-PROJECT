{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Locally installing Spark"
      ],
      "metadata": {
        "id": "0T5K6O4dXcmv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_twJIIcR50s",
        "outputId": "f171f5bc-f9a9-4f14-a937-5feb28c85976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=5d6c5565b93d047c2bd07c7c3f3aa5d9dddb83614317831acffa28b9120c15ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "        .master('local[*]') \\\n",
        "        .appName('Basics') \\\n",
        "        .getOrCreate()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:"
      ],
      "metadata": {
        "id": "If3nOAZHXhgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MatrixVectorMultiplication\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Input matrix and vector\n",
        "matrix = [\n",
        "    [14, 15, 9],\n",
        "    [24, 25, 6],\n",
        "    [31, 41, 12]\n",
        "]\n",
        "\n",
        "vector = [11, 22, 33]\n",
        "\n",
        "# Define the multiplication function\n",
        "def multiply(row):\n",
        "    matrix_row, values = row\n",
        "    result = sum(value * vector[i] for i, value in enumerate(values))\n",
        "    return (matrix_row, result)\n",
        "\n",
        "# Parallelize the matrix\n",
        "matrix_rdd = spark.sparkContext.parallelize(enumerate(matrix))\n",
        "\n",
        "# Perform matrix-vector multiplication\n",
        "result = matrix_rdd.map(multiply)\n",
        "\n",
        "# Collect the result and print\n",
        "print(result.collect())\n",
        "\n",
        "# Stop the Spark Session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KJcYimETXJf",
        "outputId": "864c34a1-6ec2-4907-f395-2f3d41ce9bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 781), (1, 1012), (2, 1639)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2:"
      ],
      "metadata": {
        "id": "Wepo_d6yXkLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from math import sqrt\n",
        "\n",
        "# Dummy input data\n",
        "input_data = [\n",
        "    'key1\\t11',\n",
        "    'key2\\t21',\n",
        "    'key1\\t33',\n",
        "    'key2\\t44',\n",
        "    'key1\\t55',\n",
        "    'key2\\t66',\n",
        "]\n",
        "\n",
        "def map_func(line):\n",
        "    key, value = line.split('\\t')\n",
        "    return key, float(value)\n",
        "\n",
        "def reduce_func(data):\n",
        "    values = list(data)  # Convert data to list for clarity\n",
        "    mean_val = sum(values) / len(values)\n",
        "    sum_val = sum(values)\n",
        "    if len(values) > 1:  # Check if there are more than one value for calculation\n",
        "        std_dev_val = sqrt(sum((x - mean_val) ** 2 for x in values) / (len(values) - 1))\n",
        "    else:\n",
        "        std_dev_val = 0\n",
        "    return {\n",
        "        'mean': mean_val,\n",
        "        'sum': sum_val,\n",
        "        'std_dev': std_dev_val\n",
        "    }\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    sc = SparkContext('local', 'AggregationSpark')\n",
        "    try:\n",
        "        lines = sc.parallelize(input_data)\n",
        "        mapped = lines.map(map_func)\n",
        "        grouped = mapped.groupByKey()\n",
        "        result = grouped.mapValues(list).mapValues(reduce_func)\n",
        "        output = result.collect()\n",
        "        for key, value in output:\n",
        "            print(f'{key}\\t{value}')\n",
        "    finally:\n",
        "        sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDub3aF9URyz",
        "outputId": "40946b03-6b1e-4f5b-ad82-25027d6ef89a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "key1\t{'mean': 33.0, 'sum': 99.0, 'std_dev': 22.0}\n",
            "key2\t{'mean': 43.666666666666664, 'sum': 131.0, 'std_dev': 22.50185177565023}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3:"
      ],
      "metadata": {
        "id": "8LR4plM5Xmf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SortData\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define dummy input data\n",
        "dummy_data = [\n",
        "    \"3\\tCycle\",\n",
        "    \"1\\tBat\",\n",
        "    \"2\\tChainsaw\",\n",
        "    \"4\\tElephant\"\n",
        "]\n",
        "\n",
        "# Create RDD from dummy data\n",
        "data_rdd = spark.sparkContext.parallelize(dummy_data)\n",
        "\n",
        "# Sort the data based on the first column\n",
        "sorted_data = data_rdd.sortBy(lambda x: x.split('\\t')[0])\n",
        "\n",
        "# Collect and print the sorted data\n",
        "sorted_results = sorted_data.collect()\n",
        "for result in sorted_results:\n",
        "    print(result)\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmcVxSlOW3bJ",
        "outputId": "8fb24c02-1936-416a-a408-25761c1a26cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\tBat\n",
            "2\tChainsaw\n",
            "3\tCycle\n",
            "4\tElephant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4:"
      ],
      "metadata": {
        "id": "vk8uqZgXXonK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create a Spark context\n",
        "conf = SparkConf().setAppName(\"SearchElement\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Define the data to be searched\n",
        "data = [210,310,456,588,329,514]\n",
        "\n",
        "# Parallelize the data into RDD (Resilient Distributed Dataset)\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Define the search function\n",
        "def search_element(element):\n",
        "    return element == 588  # Change the search element as needed\n",
        "\n",
        "# Map function to search for the element in the dataset\n",
        "result = rdd.map(search_element)\n",
        "\n",
        "# Collect the results\n",
        "search_result = result.collect()\n",
        "\n",
        "# Print the search result\n",
        "if True in search_result:\n",
        "    print(\"Element found in the dataset\")\n",
        "else:\n",
        "    print(\"Element not found in the dataset\")\n",
        "\n",
        "# Stop the Spark context\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJjy2ya8W6Ik",
        "outputId": "036f3028-33a9-46c3-f6ca-0a3bde9c85e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Element found in the dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5:"
      ],
      "metadata": {
        "id": "arorRCqqXqgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"Joins\")\n",
        "\n",
        "# Create RDDs for left and right datasets\n",
        "left_data = sc.parallelize([(1, \"P\"), (2, \"Q\"), (3, \"R\")])\n",
        "right_data = sc.parallelize([(1, \"X\"), (3, \"Y\"), (4, \"Z\")])\n",
        "\n",
        "# Perform map-side join\n",
        "map_join = left_data.join(right_data)\n",
        "\n",
        "# Perform reduce-side join\n",
        "reduce_join = left_data.union(right_data).reduceByKey(lambda x, y: (x, y))\n",
        "\n",
        "# Print the results\n",
        "print(\"Map Side Join:\", map_join.collect())\n",
        "print(\"Reduce Side Join:\", reduce_join.collect())\n",
        "\n",
        "# Stop SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb6rNQjCXZ0h",
        "outputId": "96b499cc-7846-4fdf-a222-012026080539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Map Side Join: [(1, ('P', 'X')), (3, ('R', 'Y'))]\n",
            "Reduce Side Join: [(2, 'Q'), (4, 'Z'), (1, ('P', 'X')), (3, ('R', 'Y'))]\n"
          ]
        }
      ]
    }
  ]
}